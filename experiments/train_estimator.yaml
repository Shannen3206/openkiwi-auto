### Train Predictor Estimator ###

model: estimator

# Model Files will be saved here
output-dir: F:/openkiwi/runs/estimator/target

#### MODEL SPECIFIC OPTS ####

## ESTIMATOR ##

# If load-model points to a pretrained Estimator,
# These settings are ignored.

# LSTM Settings
hidden-est: 125
rnn-layers-est: 1
dropout-est: 0.0
# Use linear layer to reduce dimension prior to LSTM
mlp-est: True

# Multitask Learning Settings #

# Continue training the predictor on the postedited text.
# If set, will do an additional forward pass through the predictor
# Using the SRC, PE pair and add the `Predictor` loss for the tokens in the
# postedited text PE. Recommended if you have access to PE
# Requires setting train-pe, valid-pe
token-level: True
# Predict Sentence Level Scores
# Requires setting train-sentence-scores, valid-sentence-scores
sentence-level: true
# Use probabilistic Loss for sentence scores instead of squared error.
# If set, the model will output mean and variance of a truncated Gaussian
# distribution over the interval [0, 1], and use log-likelihood loss instead
# of mean squared error.
# Seems to improve performance
sentence-ll: False
# Predict Binary Label for each sentence, indicating hter == 0.0
# Requires setting train-sentence-scores, valid-sentence-scores
binary-level: False

# WMT 18 Format Settings #

# Predict target tags. Requires train-target-tags, valid-target-tags to be set.
predict-target: false
target-bad-weight: 2.5
# Predict source tags. Requires train-source-tags, valid-source-tags to be set.
predict-source: false
source-bad-weight: 2.5
# Predict gap tags. Requires train-target-tags, valid-target-tags to be set.
# and wmt18-format set to true
predict-gaps: false
#target-bad-weight: 2.5


### TRAIN OPTS ###
epochs: 1
# Additionally Eval and checkpoint every n training steps
# Explicitly disable by setting to zero (default)
checkpoint-validation-steps: 5000
# If False, never save the Models
checkpoint-save: true
# Keep Only the n best models according to the main metric (F1Mult by default)
# USeful to avoid filling the harddrive during a long run
checkpoint-keep-only-best: 1
# If greater than zero, Early Stop after n evaluation cycles without improvement
checkpoint-early-stop-patience: 0


# Print Train Stats Every n batches
log-interval: 100
# LR. Currently ADAM is only optimizer supported.
# 1e-3 * (batch_size / 32) seems to work well
learning-rate: 2e-3

train-batch-size: 64
valid-batch-size: 64



### LOADING ###

# Load pretrained (sub-)model.
# If set, the model architecture params are ignored.
# As the vocabulary of the pretrained model will be used,
# all vocab-params will also be ignored.

# (i) load-pred-source or load-pred-target: Predictor instance
#     -> a new Estimator is initialized with the given predictor(s).
# (ii) load-model: Estimator instance.
#                  As the Predictor is a submodule of the Estimator,
#                  load-pred-{source,target} will be ignored if this is set.

# load-model: path_to_estimator
# load-pred-source: path_to_predictor_source_target
load-pred-target: F:/openkiwi/runs/predictor/target/best_model.torch


###  DATA ###

# Set to True to use target_tags in WMT18 format
wmt18-format: false

train-source: F:/data/wmt2017/word_level/en_de/train/small500/train.src
train-target: F:/data/wmt2017/word_level/en_de/train/small500/train.mt
train-pe: F:/data/wmt2017/word_level/en_de/train/small500/train.pe
train-target-tags: F:/data/wmt2017/word_level/en_de/train/small500/train.tags
train-sentence-scores: F:/data/wmt2017/sent_level/en_de/train/small500/train.hter


valid-source: F:/data/wmt2017/word_level/en_de/dev/small500/dev.src
valid-target: F:/data/wmt2017/word_level/en_de/dev/small500/dev.mt
valid-pe: F:/data/wmt2017/word_level/en_de/dev/small500/dev.pe
valid-target-tags: F:/data/wmt2017/word_level/en_de/dev/small500/dev.tags
valid-sentence-scores: F:/data/wmt2017/sent_level/en_de/dev/small500/dev.hter


### GENERAL OPTS ###

# Experiment Name for MLFlow
experiment-name: EN-DE Train Estimator
# Do not set or set to negative number for CPU
gpu-id: -1
